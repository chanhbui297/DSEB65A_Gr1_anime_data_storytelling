{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Anime Dataset 2023 - Advanced Exploratory Data Analysis\n", "\n", "This notebook performs a thorough and academically oriented EDA for the Anime Dataset 2023.\n", "\n", "Role and goal:\n", "- Role: Producer and data analyst\n", "- Goal: Understand which factors are associated with anime success and prepare features for a later Success Score model\n", "\n", "Target metric (conceptual model):\n", "- Score = a*Type + b*Episodes + c*Genre_Action + ... + constant + error\n", "\n", "The notebook is structured with clear sections, explicit assumptions and systematic diagnostics.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Data understanding and feature dictionary\n", "\n", "This section documents the meaning of each column in the dataset and groups features into logical categories.\n", "The content below is adapted from the original raw analysis file.\n"]}, {"cell_type": "markdown", "id": "1fc2b0a0", "metadata": {"id": "1fc2b0a0"}, "source": ["## Definition of Each Features"]}, {"cell_type": "markdown", "id": "ceb20358", "metadata": {"id": "ceb20358"}, "source": ["### 1. Th\u00f4ng tin c\u01a1 b\u1ea3n v\u00e0 nh\u1eadn d\u1ea1ng (Basic Identification & Description)\n", "- **anime_id**: ID duy nh\u1ea5t cho m\u1ed7i anime.\n", "- **Name**: T\u00ean c\u1ee7a anime b\u1eb1ng ng\u00f4n ng\u1eef g\u1ed1c.\n", "- **English name**: T\u00ean ti\u1ebfng Anh c\u1ee7a anime.\n", "- **Other name**: T\u00ean b\u1ea3n \u0111\u1ecba ho\u1eb7c t\u1ef1a \u0111\u1ec1 c\u1ee7a anime.\n", "- **Synopsis**: M\u00f4 t\u1ea3 ho\u1eb7c t\u00f3m t\u1eaft ng\u1eafn g\u1ecdn v\u1ec1 c\u1ed1t truy\u1ec7n c\u1ee7a anime.\n", "- **Genres**: C\u00e1c th\u1ec3 lo\u1ea1i c\u1ee7a anime, \u0111\u01b0\u1ee3c ph\u00e2n t\u00e1ch b\u1eb1ng d\u1ea5u ph\u1ea9y.\n", "- **Image URL**: URL c\u1ee7a h\u00ecnh \u1ea3nh ho\u1eb7c poster c\u1ee7a anime.\n", "\n", "### 2. Chi ti\u1ebft s\u1ea3n xu\u1ea5t v\u00e0 k\u1ef9 thu\u1eadt (Production & Technical Details)\n", "- **Type**: Lo\u1ea1i anime.\n", "- **Source**: V\u1eadt li\u1ec7u g\u1ed1c c\u1ee7a anime.\n", "- **Producers**: C\u00e1c c\u00f4ng ty s\u1ea3n xu\u1ea5t ho\u1eb7c nh\u00e0 s\u1ea3n xu\u1ea5t c\u1ee7a anime.\n", "- **Studios**: C\u00e1c studio ho\u1ea1t h\u00ecnh \u0111\u00e3 th\u1ef1c hi\u1ec7n anime.\n", "- **Licensors**: C\u00e1c nh\u00e0 c\u1ea5p ph\u00e9p c\u1ee7a anime.\n", "- **Episodes**: S\u1ed1 l\u01b0\u1ee3ng t\u1eadp trong anime.\n", "- **Duration**: Th\u1eddi l\u01b0\u1ee3ng c\u1ee7a m\u1ed7i t\u1eadp phim.\n", "\n", "### 3. Th\u00f4ng tin ph\u00e1t s\u00f3ng v\u00e0 ph\u00e1t h\u00e0nh (Release & Airing Information)\n", "- **Aired**: Ng\u00e0y anime \u0111\u01b0\u1ee3c ph\u00e1t s\u00f3ng.\n", "- **Premiered**: M\u00f9a v\u00e0 n\u0103m anime ra m\u1eaft.\n", "- **Status**: Tr\u1ea1ng th\u00e1i c\u1ee7a anime.\n", "\n", "### 4. Ch\u1ec9 s\u1ed1 t\u01b0\u01a1ng t\u00e1c ng\u01b0\u1eddi xem v\u00e0 hi\u1ec7u su\u1ea5t (Audience Engagement & Performance Metrics)\n", "- **Score**: \u0110i\u1ec3m \u0111\u01b0\u1ee3c trao cho anime.\n", "- **Rating**: X\u1ebfp h\u1ea1ng \u0111\u1ed9 tu\u1ed5i c\u1ee7a anime.\n", "- **Rank**: X\u1ebfp h\u1ea1ng c\u1ee7a anime d\u1ef1a tr\u00ean m\u1ee9c \u0111\u1ed9 ph\u1ed5 bi\u1ebfn ho\u1eb7c c\u00e1c ti\u00eau ch\u00ed kh\u00e1c.\n", "- **Popularity**: X\u1ebfp h\u1ea1ng m\u1ee9c \u0111\u1ed9 ph\u1ed5 bi\u1ebfn c\u1ee7a anime.\n", "- **Favorites**: S\u1ed1 l\u1ea7n anime \u0111\u01b0\u1ee3c ng\u01b0\u1eddi d\u00f9ng \u0111\u00e1nh d\u1ea5u l\u00e0 y\u00eau th\u00edch.\n", "- **Scored By**: S\u1ed1 l\u01b0\u1ee3ng ng\u01b0\u1eddi d\u00f9ng \u0111\u00e3 ch\u1ea5m \u0111i\u1ec3m anime.\n", "- **Members**: S\u1ed1 l\u01b0\u1ee3ng th\u00e0nh vi\u00ean \u0111\u00e3 th\u00eam anime v\u00e0o danh s\u00e1ch c\u1ee7a h\u1ecd tr\u00ean n\u1ec1n t\u1ea3ng."]}, {"cell_type": "markdown", "id": "6ba06309", "metadata": {"id": "6ba06309"}, "source": ["### Unnecessary Feature\n", "- Ti\u1ebfn h\u00e0nh drop feature kh\u00f4ng c\u1ea7n thi\u1ebft kh\u1ecfi dataframe"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Setup and configuration\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os\n", "import re\n", "from itertools import chain\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import plotly.express as px\n", "\n", "pd.set_option(\"display.max_columns\", 120)\n", "pd.set_option(\"display.width\", 180)\n", "\n", "PLOT_DIR = \"plots\"\n", "os.makedirs(PLOT_DIR, exist_ok=True)\n", "\n", "def save_plot(name: str):\n", "    \"\"\"Save static matplotlib plots in a consistent way.\"\"\"\n", "    plt.tight_layout()\n", "    path = os.path.join(PLOT_DIR, f\"{name}.png\")\n", "    plt.savefig(path, dpi=150)\n", "    plt.close()\n", "    print(f\"Saved plot to {path}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. EDA methodology\n", "\n", "This EDA follows a structured workflow:\n", "\n", "1. Structural overview\n", "   - Inspect shape, schema and basic distributions.\n", "2. Data quality assessment\n", "   - Missingness, NA like tokens, suspicious zeros, type conversions and outliers.\n", "3. Univariate analysis\n", "   - Distributions of key numeric and categorical features.\n", "4. Bivariate analysis with respect to the target\n", "   - Score vs numeric features\n", "   - Score vs categorical features\n", "5. Multivariate structure\n", "   - Correlation matrix and simple multicollinearity diagnostics.\n", "6. Segment based and diagnostic analysis\n", "   - Year, Type and Studio segments.\n", "7. Feature readiness\n", "   - Document which features can be safely used for modeling.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Load dataset and structural overview"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["DATA_PATH = \"anime-dataset-2023.csv\"  # update if needed\n", "\n", "df = pd.read_csv(DATA_PATH)\n", "\n", "print(\"Shape (rows, columns):\", df.shape)\n", "print(\"\\nColumns:\")\n", "print(df.columns.tolist())\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["print(\"First 10 rows:\")\n", "display(df.head(10))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["print(\"Numeric columns summary:\")\n", "display(df.describe(include=[np.number]).T)\n", "\n", "print(\"\\nCategorical columns summary:\")\n", "display(df.describe(include=[\"object\"]).T)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Data quality assessment\n", "\n", "This section focuses on data quality:\n", "- Missing values and NA like tokens\n", "- Duplicates\n", "- Type conversions\n", "- Suspicious zeros\n", "- Outlier inspection\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.1 Missing values by column"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["missing_ratio = df.isna().mean().sort_values(ascending=False)\n", "print(\"Missing value ratio by column:\")\n", "display(missing_ratio)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.2 Duplicates by anime_id"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["dup_count = df.duplicated(subset=[\"anime_id\"]).sum()\n", "print(\"Number of duplicated anime_id:\", dup_count)\n", "\n", "if dup_count > 0:\n", "    print(\"\\nExamples of duplicated anime_id:\")\n", "    display(df[df.duplicated(subset=[\"anime_id\"], keep=False)].sort_values(\"anime_id\").head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.3 NA like tokens in object columns\n", "\n", "Some string tokens effectively represent missing values but are not coded as NaN.\n", "We detect and optionally normalize these tokens to proper NaN.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "\n", "NA_TOKENS = {\n", "    \"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\",\n", "    \"None\", \"NONE\", \"null\", \"NULL\", \"NaN\", \"nan\",\n", "    \"-\", \"?\", \"Unknown\", \"unknown\"\n", "}\n", "\n", "obj_cols = df.select_dtypes(include=[\"object\"]).columns\n", "\n", "na_like_rows = []\n", "for col in obj_cols:\n", "    vc = df[col].value_counts(dropna=False)\n", "    top_vals = vc.head(20)\n", "    na_like_vals = [v for v in top_vals.index if isinstance(v, str) and v.strip() in NA_TOKENS]\n", "    if na_like_vals:\n", "        na_like_rows.append({\n", "            \"column\": col,\n", "            \"na_like_values\": na_like_vals,\n", "            \"na_like_total\": int(top_vals.loc[na_like_vals].sum())\n", "        })\n", "\n", "na_like_report = pd.DataFrame(na_like_rows)\n", "print(\"Columns with NA like tokens (top 20 values inspected):\")\n", "display(na_like_report)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Normalize NA like tokens to real NaN\n", "\n", "for col in obj_cols:\n", "    df[col] = df[col].replace(list(NA_TOKENS), np.nan)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.4 Type conversion for Score and Episodes"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Convert Score to numeric\n", "df[\"Score\"] = pd.to_numeric(df[\"Score\"], errors=\"coerce\")\n", "\n", "# Preserve original Episodes\n", "df[\"Episodes_raw\"] = df[\"Episodes\"]\n", "df[\"Episodes\"] = pd.to_numeric(df[\"Episodes\"], errors=\"coerce\")\n", "\n", "print(\"Episodes before and after conversion (sample):\")\n", "display(df[[\"Episodes_raw\", \"Episodes\"]].head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.5 Suspicious zeros in numeric columns\n", "\n", "Some numeric columns may use 0 as a placeholder for missing or not applicable values.\n", "We generate a zero report to identify candidates for zero to NaN recoding.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["num_cols = df.select_dtypes(include=[np.number]).columns\n", "\n", "rows = []\n", "n_total = len(df)\n", "for c in num_cols:\n", "    series = df[c]\n", "    zero_count = int((series == 0).sum())\n", "    nan_count = int(series.isna().sum())\n", "    rows.append({\n", "        \"column\": c,\n", "        \"zero_count\": zero_count,\n", "        \"zero_pct\": round(zero_count / n_total * 100, 2) if n_total else 0.0,\n", "        \"nan_count\": nan_count,\n", "    })\n", "\n", "zero_report = pd.DataFrame(rows).sort_values([\"zero_pct\", \"zero_count\"], ascending=False)\n", "print(\"Zero report for numeric columns:\")\n", "display(zero_report.head(30))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Example: specify which columns should treat zero as missing if business logic supports it\n", "cols_treat_zero_as_nan = []  # for example: [\"Rank\", \"Popularity\"]\n", "\n", "for c in cols_treat_zero_as_nan:\n", "    if c in df.columns:\n", "        df[c] = df[c].replace(0, np.nan)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.6 Outlier inspection for Score and Episodes"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["print(\"Score summary:\")\n", "display(df[\"Score\"].describe())\n", "\n", "print(\"\\nEpisodes summary:\")\n", "display(df[\"Episodes\"].describe())\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "df[\"Score\"].hist(bins=30)\n", "plt.xlabel(\"Score\")\n", "plt.ylabel(\"Count\")\n", "plt.title(\"Distribution of Score\")\n", "save_plot(\"score_distribution\")\n", "plt.show()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "df[\"Episodes\"].hist(bins=50)\n", "plt.xlabel(\"Episodes\")\n", "plt.ylabel(\"Count\")\n", "plt.title(\"Distribution of Episodes\")\n", "save_plot(\"episodes_distribution\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.7 IQR based outlier analysis\n", "\n", "We use the Interquartile Range (IQR) rule to identify extreme values for key numeric\n", "variables. A value is considered an outlier if it lies outside:\n", "\n", "[Q1 - 1.5 * IQR, Q3 + 1.5 * IQR].\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def find_outliers_iqr(series: pd.Series):\n", "    series = series.dropna()\n", "    Q1 = series.quantile(0.25)\n", "    Q3 = series.quantile(0.75)\n", "    IQR = Q3 - Q1\n", "    lower = Q1 - 1.5 * IQR\n", "    upper = Q3 + 1.5 * IQR\n", "    mask = (series < lower) | (series > upper)\n", "    return series[mask], lower, upper\n", "\n", "key_numeric = [\"Score\", \"Members\", \"Favorites\", \"Episodes\"]\n", "for col in key_numeric:\n", "    if col in df.columns:\n", "        print(f\"\\n=== IQR outliers for {col} ===\")\n", "        outliers, lower, upper = find_outliers_iqr(df[col])\n", "        print(f\"Bounds: [{lower:.3f}, {upper:.3f}]\")\n", "        print(f\"Detected {len(outliers)} outliers out of {df[col].notna().sum()} non-missing values.\")\n", "        # Show a few extreme high outliers\n", "        if len(outliers) > 0:\n", "            extreme_high = outliers.sort_values(ascending=False).head(10)\n", "            print(\"Top 10 extreme high values:\")\n", "            display(df.loc[extreme_high.index, [\"Name\", col, \"Score\", \"Type\"]].head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.8 String data consistency: fuzzy matching for studios\n", "\n", "String fields such as Studios or Producers may contain near duplicates\n", "due to inconsistent naming (for example, \"Sunrise\" vs \"Sunrise Inc.\").\n", "We use fuzzy string matching to detect candidates for standardization.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["try:\n", "    from rapidfuzz import fuzz\n", "except ImportError:\n", "    import sys\n", "    !{sys.executable} -m pip install rapidfuzz -q\n", "    from rapidfuzz import fuzz\n", "\n", "if \"Studios\" in df.columns:\n", "    unique_studios_raw = df[\"Studios\"].dropna().unique()\n", "    studio_tokens = set()\n", "    for s in unique_studios_raw:\n", "        for part in str(s).split(\",\"):\n", "            token = part.strip()\n", "            if token:\n", "                studio_tokens.add(token)\n", "\n", "    studio_list = sorted(studio_tokens)\n", "    print(f\"Total unique studio tokens: {len(studio_list)}\")\n", "\n", "    from itertools import combinations\n", "\n", "    similar_pairs = []\n", "    max_pairs = 500  # limit combinations for performance\n", "    subset = studio_list[:max_pairs]\n", "\n", "    for s1, s2 in combinations(subset, 2):\n", "        score = fuzz.ratio(s1, s2)\n", "        if score > 90 and s1 != s2:\n", "            similar_pairs.append((s1, s2, score))\n", "\n", "    if similar_pairs:\n", "        similar_pairs.sort(key=lambda x: x[2], reverse=True)\n", "        print(\"Potential duplicate / inconsistent studio names (similarity > 90):\")\n", "        for s1, s2, sc in similar_pairs[:20]:\n", "            print(f\"- '{s1}' vs '{s2}' (Similarity: {sc:.1f})\")\n", "    else:\n", "        print(\"No highly similar studio name pairs found in the checked subset.\")\n", "else:\n", "    print(\"Studios column not available.\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.9 Business rule and logical validation\n", "\n", "We validate several basic business rules to detect logically inconsistent records:\n", "\n", "- Aired end date should not be before the start date.\n", "- The count of users who scored an anime (Scored By) should not exceed Members.\n", "- Duration should be roughly consistent with the number of episodes.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# 1. Aired start and end dates\n", "\n", "if \"Aired\" in df.columns:\n", "    aired_split = df[\"Aired\"].str.split(\" to \", n=1, expand=True)\n", "    start_dates = pd.to_datetime(aired_split[0], errors=\"coerce\")\n", "    end_dates = pd.to_datetime(aired_split[1], errors=\"coerce\")\n", "\n", "    invalid_dates_mask = (start_dates.notna()) & (end_dates.notna()) & (end_dates < start_dates)\n", "    invalid_dates = df[invalid_dates_mask]\n", "\n", "    print(f\"Found {len(invalid_dates)} entries with end date before start date.\")\n", "    if len(invalid_dates) > 0:\n", "        display(invalid_dates[[\"Name\", \"Aired\", \"Score\", \"Type\"]].head(10))\n", "\n", "# 2. Scored By vs Members\n", "\n", "if {\"Scored By\", \"Members\"} <= set(df.columns):\n", "    scored_by_num = pd.to_numeric(df[\"Scored By\"], errors=\"coerce\") if df[\"Scored By\"].dtype == object else df[\"Scored By\"]\n", "    members_num = pd.to_numeric(df[\"Members\"], errors=\"coerce\") if df[\"Members\"].dtype == object else df[\"Members\"]\n", "\n", "    invalid_scores_mask = scored_by_num > members_num\n", "    invalid_scores = df[invalid_scores_mask]\n", "\n", "    print(f\"Found {len(invalid_scores)} entries where 'Scored By' > 'Members'.\")\n", "    if len(invalid_scores) > 0:\n", "        display(invalid_scores[[\"Name\", \"Score\", \"Members\", \"Scored By\"]].head(10))\n", "\n", "# 3. Duration vs Episodes (simple heuristic)\n", "\n", "if {\"Duration\", \"Episodes\"} <= set(df.columns):\n", "    dur = df[\"Duration\"].astype(str)\n", "    ep = df[\"Episodes\"]\n", "\n", "    # Example rule: Episodes == 1 but duration string contains 'per ep'\n", "    mask_single_with_per_ep = (ep == 1) & dur.str.contains(\"per ep\", case=False, na=False)\n", "    inconsistent_single = df[mask_single_with_per_ep]\n", "\n", "    print(f\"Found {len(inconsistent_single)} entries where Episodes == 1 but Duration looks like per-episode duration.\")\n", "    if len(inconsistent_single) > 0:\n", "        display(inconsistent_single[[\"Name\", \"Episodes\", \"Duration\"]].head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Univariate analysis\n", "\n", "We study the distribution of individual variables to understand their scale, skewness and potential issues.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 6.1 Numeric features"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["numeric_candidates = [\"Score\", \"Episodes\", \"Members\", \"Scored By\", \"Favorites\"]\n", "numeric_existing = [c for c in numeric_candidates if c in df.columns]\n", "\n", "df[numeric_existing].describe().T\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for col in numeric_existing:\n", "    plt.figure()\n", "    df[col].hist(bins=40)\n", "    plt.title(f\"Distribution of {col}\")\n", "    plt.xlabel(col)\n", "    plt.ylabel(\"Count\")\n", "    save_plot(f\"dist_{col.lower()}\")\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 6.2 Categorical features"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["categorical_candidates = [\"Type\", \"Rating\", \"Status\", \"Source\"]\n", "categorical_existing = [c for c in categorical_candidates if c in df.columns]\n", "\n", "for col in categorical_existing:\n", "    print(f\"\\nValue counts for {col}:\")\n", "    display(df[col].value_counts(dropna=False).head(20))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for col in categorical_existing:\n", "    vc = df[col].value_counts().head(15)\n", "    plt.figure(figsize=(8, 4))\n", "    vc.plot(kind=\"bar\")\n", "    plt.title(f\"Top categories for {col}\")\n", "    plt.ylabel(\"Count\")\n", "    save_plot(f\"cat_{col.lower()}\")\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Target variable analysis: Score\n", "\n", "We treat Score as the primary success metric.\n", "We create ordered bands and inspect the distribution.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["bins = [0, 6, 7, 8, 10]\n", "labels = [\"Low\", \"Medium\", \"High\", \"Top\"]\n", "df[\"Score_band\"] = pd.cut(df[\"Score\"], bins=bins, labels=labels, include_lowest=True)\n", "\n", "print(\"Score band distribution (count):\")\n", "display(df[\"Score_band\"].value_counts(dropna=False))\n", "\n", "print(\"\\nScore band distribution (ratio):\")\n", "display(df[\"Score_band\"].value_counts(normalize=True, dropna=False))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "df.boxplot(column=\"Score\")\n", "plt.title(\"Boxplot of Score\")\n", "save_plot(\"score_boxplot\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Bivariate analysis with Score\n", "\n", "We now explore how Score relates to other variables:\n", "- Numeric features (Episodes, Members, Scored By, Favorites)\n", "- Categorical features (Type, Rating, Source, Score_band segments)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 8.1 Score vs numeric features"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for col in [\"Episodes\", \"Members\", \"Scored By\", \"Favorites\"]:\n", "    if col in df.columns:\n", "        fig = px.scatter(\n", "            df,\n", "            x=col,\n", "            y=\"Score\",\n", "            title=f\"Score vs {col}\",\n", "            opacity=0.5\n", "        )\n", "        fig.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 8.2 Score by categorical features"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for col in [\"Type\", \"Rating\", \"Source\"]:\n", "    if col in df.columns:\n", "        print(f\"\\nScore by {col}:\")\n", "        stats = df.groupby(col)[\"Score\"].describe().sort_values(\"mean\", ascending=False)\n", "        display(stats.head(20))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for col in [\"Type\", \"Rating\", \"Source\"]:\n", "    if col in df.columns:\n", "        plt.figure(figsize=(10, 5))\n", "        df.boxplot(column=\"Score\", by=col, rot=45)\n", "        plt.suptitle(\"\")\n", "        plt.title(f\"Score by {col}\")\n", "        plt.ylabel(\"Score\")\n", "        save_plot(f\"score_by_{col.lower()}\")\n", "        plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 8.3 Segmented distribution analysis\n", "\n", "We examine how the distribution of key variables changes across important segments,\n", "such as Type and Rating.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Score distribution by Type (only Types with sufficient sample size)\n", "\n", "if {\"Score\", \"Type\"} <= set(df.columns):\n", "    df_seg = df.dropna(subset=[\"Score\", \"Type\"]).copy()\n", "    type_counts = df_seg[\"Type\"].value_counts()\n", "    major_types = type_counts[type_counts > 100].index\n", "    df_seg = df_seg[df_seg[\"Type\"].isin(major_types)]\n", "\n", "    print(\"Types included in segmented analysis (n > 100):\")\n", "    display(major_types)\n", "\n", "    fig = px.box(\n", "        df_seg,\n", "        x=\"Type\",\n", "        y=\"Score\",\n", "        title=\"Score distribution by Type\",\n", "        labels={\"Type\": \"Anime Type\", \"Score\": \"User Score\"}\n", "    )\n", "    fig.show()\n", "\n", "# log(Members) distribution by Rating\n", "\n", "if {\"Members\", \"Rating\"} <= set(df.columns):\n", "    df_seg2 = df.dropna(subset=[\"Members\", \"Rating\"]).copy()\n", "    df_seg2 = df_seg2[df_seg2[\"Members\"] > 0]\n", "    df_seg2[\"log_Members\"] = np.log10(df_seg2[\"Members\"])\n", "\n", "    rating_counts = df_seg2[\"Rating\"].value_counts()\n", "    major_ratings = rating_counts[rating_counts > 100].index\n", "    df_seg2 = df_seg2[df_seg2[\"Rating\"].isin(major_ratings)]\n", "\n", "    print(\"Ratings included in segmented analysis (n > 100):\")\n", "    display(major_ratings)\n", "\n", "    fig = px.box(\n", "        df_seg2,\n", "        x=\"Rating\",\n", "        y=\"log_Members\",\n", "        title=\"log10(Members) distribution by Rating\",\n", "        labels={\"Rating\": \"Age Rating\", \"log_Members\": \"log10(Members)\"}\n", "    )\n", "    fig.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Multivariate structure and correlation\n", "\n", "Here we look at the joint structure of numeric features:\n", "- Correlation matrix\n", "- Simple multicollinearity diagnostics\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["num_for_corr = df.select_dtypes(include=[np.number]).copy()\n", "corr_matrix = num_for_corr.corr()\n", "\n", "print(\"Correlation matrix (numeric features):\")\n", "display(corr_matrix)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "im = plt.imshow(corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n", "plt.colorbar(im, fraction=0.046, pad=0.04)\n", "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n", "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n", "plt.title(\"Correlation matrix of numeric features\")\n", "plt.tight_layout()\n", "save_plot(\"corr_matrix_numeric\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Genres exploration\n", "\n", "We transform the Genres string column into a list of genres and corresponding dummy variables, then study:\n", "- Genre frequencies\n", "- Score by genre\n", "- Distribution of number of genres per title\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def split_genres(value):\n", "    if pd.isna(value):\n", "        return []\n", "    parts = [g.strip() for g in str(value).split(\",\")]\n", "    return [p for p in parts if p]\n", "\n", "df[\"Genres_list\"] = df[\"Genres\"].apply(split_genres)\n", "all_genres = sorted(set(chain.from_iterable(df[\"Genres_list\"])))\n", "print(\"Number of unique genres:\", len(all_genres))\n", "print(\"Sample genres:\", all_genres[:20])\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["for g in all_genres:\n", "    col_name = f\"Genre_{re.sub(r'[^0-9A-Za-z]+', '_', g)}\"\n", "    df[col_name] = df[\"Genres_list\"].apply(lambda lst, genre=g: int(genre in lst))\n", "\n", "genre_cols = [c for c in df.columns if c.startswith(\"Genre_\")]\n", "print(\"Number of genre dummy columns:\", len(genre_cols))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["genre_counts = df[genre_cols].sum().sort_values(ascending=False)\n", "print(\"Top 30 genres by frequency:\")\n", "display(genre_counts.head(30))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["genre_mean_score = pd.Series(\n", "    {g: df.loc[df[g] == 1, \"Score\"].mean() for g in genre_cols}\n", ").sort_values(ascending=False)\n", "\n", "print(\"Top 20 genres by mean Score:\")\n", "display(genre_mean_score.head(20))\n", "\n", "print(\"\\nBottom 20 genres by mean Score:\")\n", "display(genre_mean_score.tail(20))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df[\"n_genres\"] = df[\"Genres_list\"].apply(len)\n", "\n", "print(\"Number of genres per title summary:\")\n", "display(df[\"n_genres\"].describe())\n", "\n", "plt.figure()\n", "df[\"n_genres\"].hist(bins=range(1, 15))\n", "plt.xlabel(\"Number of genres\")\n", "plt.ylabel(\"Count\")\n", "plt.title(\"Distribution of number of genres per title\")\n", "save_plot(\"dist_n_genres\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Diagnostic EDA by year and studio\n", "\n", "We perform segment based analysis to understand temporal and studio related patterns.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 10.4 Genre co-occurrence and association analysis\n", "\n", "We build a co-occurrence matrix for the most common genres and compute\n", "pairwise correlations to understand which genres tend to appear together.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Build a title-genre incidence matrix\n", "\n", "if \"Genres_list\" in df.columns:\n", "    # Explode genres by title\n", "    df_genres_long = df[[\"anime_id\", \"Genres_list\"]].explode(\"Genres_list\")\n", "    df_genres_long[\"Genres_list\"] = df_genres_long[\"Genres_list\"].astype(str).str.strip()\n", "    df_genres_long = df_genres_long[df_genres_long[\"Genres_list\"] != \"\"]\n", "\n", "    co_occurrence = pd.crosstab(df_genres_long[\"anime_id\"], df_genres_long[\"Genres_list\"])\n", "\n", "    # Focus on top genres to keep the matrix interpretable\n", "    top_genres = co_occurrence.sum().nlargest(15).index\n", "    co_occurrence_top = co_occurrence[top_genres]\n", "\n", "    genre_corr = co_occurrence_top.corr()\n", "\n", "    print(\"Top genres used in co-occurrence matrix:\")\n", "    display(top_genres)\n", "\n", "    fig = px.imshow(\n", "        genre_corr,\n", "        text_auto=True,\n", "        aspect=\"auto\",\n", "        title=\"Co-occurrence correlation of top 15 genres\"\n", "    )\n", "    fig.show()\n", "else:\n", "    print(\"Genres_list not available - ensure genre preprocessing section has been executed.\")\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Year extraction\n", "\n", "if \"Aired\" in df.columns:\n", "    df[\"Year\"] = df[\"Aired\"].str.extract(r\"(\\d{4})\").astype(float)\n", "\n", "    print(\"Year value counts (first 20 years):\")\n", "    display(df[\"Year\"].value_counts().sort_index().head(20))\n", "\n", "    year_stats = df.groupby(\"Year\")[\"Score\"].agg([\"count\", \"mean\"]).dropna()\n", "    print(\"\\nScore by Year (head):\")\n", "    display(year_stats.head(20))\n", "\n", "    plt.figure(figsize=(10, 5))\n", "    year_stats[\"mean\"].plot()\n", "    plt.ylabel(\"Mean Score\")\n", "    plt.title(\"Mean Score by Year\")\n", "    save_plot(\"score_by_year_mean\")\n", "    plt.show()\n", "\n", "    plt.figure(figsize=(10, 5))\n", "    year_stats[\"count\"].plot()\n", "    plt.ylabel(\"Number of titles\")\n", "    plt.title(\"Number of anime titles by Year\")\n", "    save_plot(\"titles_by_year_count\")\n", "    plt.show()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Studio primary\n", "\n", "if \"Studios\" in df.columns:\n", "    df[\"Studios_clean\"] = df[\"Studios\"].fillna(\"Unknown\")\n", "    df[\"Studio_primary\"] = df[\"Studios_clean\"].apply(lambda x: str(x).split(\",\")[0].strip())\n", "\n", "    studio_stats = (\n", "        df.groupby(\"Studio_primary\")\n", "        .agg(\n", "            count=(\"Score\", \"count\"),\n", "            mean_score=(\"Score\", \"mean\"),\n", "        )\n", "        .sort_values(\"count\", ascending=False)\n", "    )\n", "\n", "    print(\"Top 20 studios by number of titles:\")\n", "    display(studio_stats.head(20))\n", "\n", "    min_titles = 20\n", "    big_studios = studio_stats[studio_stats[\"count\"] >= min_titles].sort_values(\"mean_score\", ascending=False)\n", "\n", "    print(f\"\\nStudios with at least {min_titles} titles, sorted by mean Score:\")\n", "    display(big_studios.head(20))\n", "\n", "    plt.figure(figsize=(10, 6))\n", "    big_studios.head(15)[\"mean_score\"].plot(kind=\"bar\")\n", "    plt.ylabel(\"Mean Score\")\n", "    plt.title(f\"Top studios by mean Score (count >= {min_titles})\")\n", "    save_plot(\"studios_top_mean_score\")\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 12. Popularity and engagement metrics\n", "\n", "We inspect standard popularity metrics and their relation to Score.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["pop_cols = [\"Score\", \"Members\", \"Scored By\", \"Favorites\"]\n", "existing_pop = [c for c in pop_cols if c in df.columns]\n", "\n", "if len(existing_pop) == len(pop_cols):\n", "    print(\"Correlation matrix for popularity metrics:\")\n", "    display(df[pop_cols].corr())\n", "\n", "    fig = px.scatter(df, x=\"Members\", y=\"Score\", title=\"Score vs Members\", opacity=0.5)\n", "    fig.show()\n", "\n", "    fig = px.scatter(df, x=\"Favorites\", y=\"Score\", title=\"Score vs Favorites\", opacity=0.5)\n", "    fig.show()\n", "\n", "    print(\"High Score but relatively low Members (top 10):\")\n", "    high_score = df[df[\"Score\"] >= 8.5]\n", "    display(high_score.sort_values(\"Members\").head(10)[[\"Name\", \"Score\", \"Members\"]])\n", "\n", "    print(\"\\nTop 10 titles by Members:\")\n", "    display(df.sort_values(\"Members\", ascending=False).head(10)[[\"Name\", \"Score\", \"Members\"]])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 13. Feature preparation summary\n", "\n", "We summarize candidate features that are ready or nearly ready for modeling.\n", "This is not a modeling step but a bridge between EDA and model development.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["genre_cols = [c for c in df.columns if c.startswith(\"Genre_\")]\n", "\n", "feature_summary = {\n", "    \"numeric_features\": [\"Episodes\", \"Members\", \"Scored By\", \"Favorites\", \"Year\"],\n", "    \"categorical_features\": [\"Type\", \"Rating\", \"Source\", \"Studio_primary\"],\n", "    \"genre_features\": genre_cols,\n", "    \"target\": \"Score\",\n", "}\n", "\n", "print(\"Feature summary:\")\n", "for key, value in feature_summary.items():\n", "    if isinstance(value, list):\n", "        existing = [c for c in value if c in df.columns]\n", "        print(f\"{key}: {len(existing)} columns used\")\n", "    else:\n", "        print(f\"{key}: {value}\")\n"]}]}